{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eeb304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c80e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 23:48:19,042 - root - INFO - AIMET\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from aimet_torch.model_preparer import prepare_model\n",
    "from aimet_torch.batch_norm_fold import fold_all_batch_norms\n",
    "from aimet_common.defs import QuantScheme\n",
    "from aimet_torch.quantsim import QuantizationSimModel\n",
    "from aimet_torch.adaround.adaround_weight import Adaround, AdaroundParameters\n",
    "\n",
    "from source.data import get_imagenet_test_loader, get_imagenet_train_val_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9164050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, dataset_loader, device='cuda', num_classes=1000):\n",
    "    def one_hot(x, K):\n",
    "        return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\n",
    "    \n",
    "    # Set BN and Droupout to eval regime\n",
    "    model.eval()\n",
    "\n",
    "    total_correct = 0\n",
    "\n",
    "    for (x, y) in tqdm(dataset_loader):\n",
    "        x = x.to(device)\n",
    "        y = one_hot(np.array(y.numpy()), num_classes)\n",
    "        target_class = np.argmax(y, axis=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(x).cpu().detach().numpy()\n",
    "            predicted_class = np.argmax(out, axis=1)\n",
    "            total_correct += np.sum(predicted_class == target_class)\n",
    "\n",
    "    total = len(dataset_loader) * dataset_loader.batch_size\n",
    "    return total_correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66282c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = get_imagenet_train_val_loaders(data_root='/gpfs/gpfs0/k.sobolev/ILSVRC-12/',\n",
    "                                       batch_size=500,\n",
    "                                       num_workers=4,\n",
    "                                       pin_memory=True,\n",
    "                                       val_perc=0.04,\n",
    "                                       shuffle=True,\n",
    "                                       random_seed=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d8ad14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = get_imagenet_test_loader(data_root='/gpfs/gpfs0/k.sobolev/ILSVRC-12/', \n",
    "                                       batch_size=500,\n",
    "                                       num_workers=4,\n",
    "                                       pin_memory=True,\n",
    "                                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b3498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_calibration_data(sim_model, use_cuda):\n",
    "    batch_size = train_loader.batch_size\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    sim_model.eval()\n",
    "    samples = 1000\n",
    "\n",
    "    batch_cntr = 0\n",
    "    with torch.no_grad():\n",
    "        for input_data, target_data in train_loader:\n",
    "\n",
    "            inputs_batch = input_data.to(device)\n",
    "            sim_model(inputs_batch)\n",
    "\n",
    "            batch_cntr += 1\n",
    "            print(batch_cntr * batch_size)\n",
    "            if (batch_cntr * batch_size) >= samples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5433224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4b65078",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a925ea32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 3 µs, total: 3 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [07:13<00:00,  1.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6976"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time \n",
    "accuracy(model, test_loader, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a3fd3e",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f838e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 23:48:33,659 - Quant - INFO - Functional         : Adding new module for node: {add} \n",
      "2022-12-21 23:48:33,660 - Quant - INFO - Reused/Duplicate   : Adding new module for node: {layer1_0_relu_1} \n",
      "2022-12-21 23:48:33,661 - Quant - INFO - Functional         : Adding new module for node: {add_1} \n",
      "2022-12-21 23:48:33,661 - Quant - INFO - Reused/Duplicate   : Adding new module for node: {layer1_1_relu_1} \n",
      "2022-12-21 23:48:33,662 - Quant - INFO - Functional         : Adding new module for node: {add_2} \n",
      "2022-12-21 23:48:33,662 - Quant - INFO - Reused/Duplicate   : Adding new module for node: {layer2_0_relu_1} \n",
      "2022-12-21 23:48:33,663 - Quant - INFO - Functional         : Adding new module for node: {add_3} \n",
      "2022-12-21 23:48:33,663 - Quant - INFO - Reused/Duplicate   : Adding new module for node: {layer2_1_relu_1} \n",
      "2022-12-21 23:48:33,664 - Quant - INFO - Functional         : Adding new module for node: {add_4} \n",
      "2022-12-21 23:48:33,664 - Quant - INFO - Reused/Duplicate   : Adding new module for node: {layer3_0_relu_1} \n",
      "2022-12-21 23:48:33,666 - Quant - INFO - Functional         : Adding new module for node: {add_5} \n",
      "2022-12-21 23:48:33,667 - Quant - INFO - Reused/Duplicate   : Adding new module for node: {layer3_1_relu_1} \n",
      "2022-12-21 23:48:33,667 - Quant - INFO - Functional         : Adding new module for node: {add_6} \n",
      "2022-12-21 23:48:33,668 - Quant - INFO - Reused/Duplicate   : Adding new module for node: {layer4_0_relu_1} \n",
      "2022-12-21 23:48:33,668 - Quant - INFO - Functional         : Adding new module for node: {add_7} \n",
      "2022-12-21 23:48:33,669 - Quant - INFO - Reused/Duplicate   : Adding new module for node: {layer4_1_relu_1} \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/fx/graph.py:606: UserWarning: Attempted to insert a call_module Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule\n",
      "  warnings.warn(\"Attempted to insert a call_module Node with \"\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebc61e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 23:48:33,948 - Utils - INFO - ...... subset to store [Conv_0, BatchNormalization_1]\n",
      "2022-12-21 23:48:33,949 - Utils - INFO - ...... subset to store [Conv_4, BatchNormalization_5]\n",
      "2022-12-21 23:48:33,949 - Utils - INFO - ...... subset to store [Conv_7, BatchNormalization_8]\n",
      "2022-12-21 23:48:33,949 - Utils - INFO - ...... subset to store [Conv_11, BatchNormalization_12]\n",
      "2022-12-21 23:48:33,950 - Utils - INFO - ...... subset to store [Conv_14, BatchNormalization_15]\n",
      "2022-12-21 23:48:33,950 - Utils - INFO - ...... subset to store [Conv_18, BatchNormalization_19]\n",
      "2022-12-21 23:48:33,951 - Utils - INFO - ...... subset to store [Conv_21, BatchNormalization_22]\n",
      "2022-12-21 23:48:33,951 - Utils - INFO - ...... subset to store [Conv_27, BatchNormalization_28]\n",
      "2022-12-21 23:48:33,951 - Utils - INFO - ...... subset to store [Conv_30, BatchNormalization_31]\n",
      "2022-12-21 23:48:33,952 - Utils - INFO - ...... subset to store [Conv_34, BatchNormalization_35]\n",
      "2022-12-21 23:48:33,952 - Utils - INFO - ...... subset to store [Conv_37, BatchNormalization_38]\n",
      "2022-12-21 23:48:33,953 - Utils - INFO - ...... subset to store [Conv_43, BatchNormalization_44]\n",
      "2022-12-21 23:48:33,961 - Utils - INFO - ...... subset to store [Conv_46, BatchNormalization_47]\n",
      "2022-12-21 23:48:33,961 - Utils - INFO - ...... subset to store [Conv_50, BatchNormalization_51]\n",
      "2022-12-21 23:48:33,961 - Utils - INFO - ...... subset to store [Conv_53, BatchNormalization_54]\n",
      "2022-12-21 23:48:33,962 - Utils - INFO - ...... subset to store [Conv_59, BatchNormalization_60]\n",
      "2022-12-21 23:48:33,962 - Utils - INFO - ...... subset to store [Conv_62, BatchNormalization_63]\n",
      "2022-12-21 23:48:33,963 - Utils - INFO - ...... subset to store [Conv_55, BatchNormalization_56]\n",
      "2022-12-21 23:48:33,963 - Utils - INFO - ...... subset to store [Conv_39, BatchNormalization_40]\n",
      "2022-12-21 23:48:33,963 - Utils - INFO - ...... subset to store [Conv_23, BatchNormalization_24]\n"
     ]
    }
   ],
   "source": [
    "_ = fold_all_batch_norms(model, input_shapes=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92826c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 23:48:37,791 - Quant - INFO - No config file provided, defaulting to config file at /usr/local/lib/python3.8/dist-packages/aimet_common/quantsim_config/default_config.json\n",
      "2022-12-21 23:48:37,809 - Quant - INFO - Unsupported op type Squeeze\n",
      "2022-12-21 23:48:37,810 - Quant - INFO - Unsupported op type Pad\n",
      "2022-12-21 23:48:37,810 - Quant - INFO - Unsupported op type Mean\n",
      "2022-12-21 23:48:37,813 - Utils - INFO - ...... subset to store [Conv_0, Relu_1]\n",
      "2022-12-21 23:48:37,814 - Utils - INFO - ...... subset to store [Conv_0, Relu_1]\n",
      "2022-12-21 23:48:37,814 - Utils - INFO - ...... subset to store [Conv_3, Relu_4]\n",
      "2022-12-21 23:48:37,815 - Utils - INFO - ...... subset to store [Conv_3, Relu_4]\n",
      "2022-12-21 23:48:37,815 - Utils - INFO - ...... subset to store [Add_6, Relu_7]\n",
      "2022-12-21 23:48:37,815 - Utils - INFO - ...... subset to store [Add_6, Relu_7]\n",
      "2022-12-21 23:48:37,816 - Utils - INFO - ...... subset to store [Conv_8, Relu_9]\n",
      "2022-12-21 23:48:37,816 - Utils - INFO - ...... subset to store [Conv_8, Relu_9]\n",
      "2022-12-21 23:48:37,817 - Utils - INFO - ...... subset to store [Add_11, Relu_12]\n",
      "2022-12-21 23:48:37,817 - Utils - INFO - ...... subset to store [Add_11, Relu_12]\n",
      "2022-12-21 23:48:37,817 - Utils - INFO - ...... subset to store [Conv_13, Relu_14]\n",
      "2022-12-21 23:48:37,818 - Utils - INFO - ...... subset to store [Conv_13, Relu_14]\n",
      "2022-12-21 23:48:37,818 - Utils - INFO - ...... subset to store [Add_17, Relu_18]\n",
      "2022-12-21 23:48:37,819 - Utils - INFO - ...... subset to store [Add_17, Relu_18]\n",
      "2022-12-21 23:48:37,819 - Utils - INFO - ...... subset to store [Conv_19, Relu_20]\n",
      "2022-12-21 23:48:37,819 - Utils - INFO - ...... subset to store [Conv_19, Relu_20]\n",
      "2022-12-21 23:48:37,820 - Utils - INFO - ...... subset to store [Add_22, Relu_23]\n",
      "2022-12-21 23:48:37,820 - Utils - INFO - ...... subset to store [Add_22, Relu_23]\n",
      "2022-12-21 23:48:37,821 - Utils - INFO - ...... subset to store [Conv_24, Relu_25]\n",
      "2022-12-21 23:48:37,821 - Utils - INFO - ...... subset to store [Conv_24, Relu_25]\n",
      "2022-12-21 23:48:37,821 - Utils - INFO - ...... subset to store [Add_28, Relu_29]\n",
      "2022-12-21 23:48:37,822 - Utils - INFO - ...... subset to store [Add_28, Relu_29]\n",
      "2022-12-21 23:48:37,822 - Utils - INFO - ...... subset to store [Conv_30, Relu_31]\n",
      "2022-12-21 23:48:37,823 - Utils - INFO - ...... subset to store [Conv_30, Relu_31]\n",
      "2022-12-21 23:48:37,823 - Utils - INFO - ...... subset to store [Add_33, Relu_34]\n",
      "2022-12-21 23:48:37,823 - Utils - INFO - ...... subset to store [Add_33, Relu_34]\n",
      "2022-12-21 23:48:37,824 - Utils - INFO - ...... subset to store [Conv_35, Relu_36]\n",
      "2022-12-21 23:48:37,824 - Utils - INFO - ...... subset to store [Conv_35, Relu_36]\n",
      "2022-12-21 23:48:37,825 - Utils - INFO - ...... subset to store [Add_39, Relu_40]\n",
      "2022-12-21 23:48:37,825 - Utils - INFO - ...... subset to store [Add_39, Relu_40]\n",
      "2022-12-21 23:48:37,825 - Utils - INFO - ...... subset to store [Conv_41, Relu_42]\n",
      "2022-12-21 23:48:37,826 - Utils - INFO - ...... subset to store [Conv_41, Relu_42]\n",
      "2022-12-21 23:48:37,826 - Utils - INFO - ...... subset to store [Add_44, Relu_45]\n",
      "2022-12-21 23:48:37,827 - Utils - INFO - ...... subset to store [Add_44, Relu_45]\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.rand(1, 3, 224, 224)    # Shape for each ImageNet sample is (3 channels) x (224 height) x (224 width)\n",
    "dummy_input = dummy_input.cuda()\n",
    "\n",
    "sim = QuantizationSimModel(model=model,\n",
    "                           quant_scheme=QuantScheme.post_training_tf_enhanced,\n",
    "                           dummy_input=dummy_input,\n",
    "                           default_output_bw=4,\n",
    "                           default_param_bw=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99467b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Quantized Model Report\n",
      "-------------------------\n",
      "----------------------------------------------------------\n",
      "Layer: conv1\n",
      "  Input[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: relu\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: maxpool\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer1.0.conv1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer1.0.relu\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer1.0.conv2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer1.1.conv1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer1.1.relu\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer1.1.conv2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: module_add\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Input[1]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: module_layer1_0_relu_1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: module_add_1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Input[1]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: module_layer1_1_relu_1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer2.0.conv1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer2.0.relu\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer2.0.conv2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer2.0.downsample.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer2.1.conv1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer2.1.relu\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer2.1.conv2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: module_add_2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Input[1]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: module_layer2_0_relu_1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: module_add_3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Input[1]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: module_layer2_1_relu_1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer3.0.conv1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer3.0.relu\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer3.0.conv2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer3.0.downsample.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer3.1.conv1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer3.1.relu\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer3.1.conv2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: module_add_4\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Input[1]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: module_layer3_0_relu_1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: module_add_5\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Input[1]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: module_layer3_1_relu_1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer4.0.conv1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer4.0.relu\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer4.0.conv2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer4.0.downsample.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer4.1.conv1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer4.1.relu\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer4.1.conv2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: module_add_6\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Input[1]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: module_layer4_0_relu_1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: module_add_7\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Input[1]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: module_layer4_1_relu_1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: avgpool\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: fc\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=4, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=4, encoding-present=False\n",
      "  -------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23dfd4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "sim.compute_encodings(forward_pass_callback=pass_calibration_data,\n",
    "                      forward_pass_callback_args=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "737b637d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "100%|██████████| 100/100 [02:15<00:00,  1.35s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06038"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "accuracy(sim.model, test_loader, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7d207",
   "metadata": {},
   "source": [
    "# AdaRound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3160e732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 21:51:46,958 - Quant - INFO - Functional         : Adding new module for node: {add} \n",
      "2022-12-21 21:51:46,959 - Quant - INFO - Reused/Duplicate   : Adding new module for node: {layer1_0_relu_1} \n",
      "2022-12-21 21:51:46,959 - Quant - INFO - Functional         : Adding new module for node: {add_1} \n",
      "2022-12-21 21:51:46,960 - Quant - INFO - Reused/Duplicate   : Adding new module for node: {layer1_1_relu_1} \n",
      "2022-12-21 21:51:46,961 - Quant - INFO - Functional         : Adding new module for node: {add_2} \n",
      "2022-12-21 21:51:46,962 - Quant - INFO - Reused/Duplicate   : Adding new module for node: {layer2_0_relu_1} \n",
      "2022-12-21 21:51:46,962 - Quant - INFO - Functional         : Adding new module for node: {add_3} \n",
      "2022-12-21 21:51:46,963 - Quant - INFO - Reused/Duplicate   : Adding new module for node: {layer2_1_relu_1} \n",
      "2022-12-21 21:51:46,964 - Quant - INFO - Functional         : Adding new module for node: {add_4} \n",
      "2022-12-21 21:51:46,964 - Quant - INFO - Reused/Duplicate   : Adding new module for node: {layer3_0_relu_1} \n",
      "2022-12-21 21:51:46,966 - Quant - INFO - Functional         : Adding new module for node: {add_5} \n",
      "2022-12-21 21:51:46,967 - Quant - INFO - Reused/Duplicate   : Adding new module for node: {layer3_1_relu_1} \n",
      "2022-12-21 21:51:46,968 - Quant - INFO - Functional         : Adding new module for node: {add_6} \n",
      "2022-12-21 21:51:46,968 - Quant - INFO - Reused/Duplicate   : Adding new module for node: {layer4_0_relu_1} \n",
      "2022-12-21 21:51:46,969 - Quant - INFO - Functional         : Adding new module for node: {add_7} \n",
      "2022-12-21 21:51:46,970 - Quant - INFO - Reused/Duplicate   : Adding new module for node: {layer4_1_relu_1} \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/fx/graph.py:606: UserWarning: Attempted to insert a call_module Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule\n",
      "  warnings.warn(\"Attempted to insert a call_module Node with \"\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4020395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 21:51:47,252 - Utils - INFO - ...... subset to store [Conv_0, BatchNormalization_1]\n",
      "2022-12-21 21:51:47,253 - Utils - INFO - ...... subset to store [Conv_4, BatchNormalization_5]\n",
      "2022-12-21 21:51:47,253 - Utils - INFO - ...... subset to store [Conv_7, BatchNormalization_8]\n",
      "2022-12-21 21:51:47,253 - Utils - INFO - ...... subset to store [Conv_11, BatchNormalization_12]\n",
      "2022-12-21 21:51:47,254 - Utils - INFO - ...... subset to store [Conv_14, BatchNormalization_15]\n",
      "2022-12-21 21:51:47,254 - Utils - INFO - ...... subset to store [Conv_18, BatchNormalization_19]\n",
      "2022-12-21 21:51:47,255 - Utils - INFO - ...... subset to store [Conv_21, BatchNormalization_22]\n",
      "2022-12-21 21:51:47,255 - Utils - INFO - ...... subset to store [Conv_27, BatchNormalization_28]\n",
      "2022-12-21 21:51:47,255 - Utils - INFO - ...... subset to store [Conv_30, BatchNormalization_31]\n",
      "2022-12-21 21:51:47,256 - Utils - INFO - ...... subset to store [Conv_34, BatchNormalization_35]\n",
      "2022-12-21 21:51:47,256 - Utils - INFO - ...... subset to store [Conv_37, BatchNormalization_38]\n",
      "2022-12-21 21:51:47,256 - Utils - INFO - ...... subset to store [Conv_43, BatchNormalization_44]\n",
      "2022-12-21 21:51:47,257 - Utils - INFO - ...... subset to store [Conv_46, BatchNormalization_47]\n",
      "2022-12-21 21:51:47,257 - Utils - INFO - ...... subset to store [Conv_50, BatchNormalization_51]\n",
      "2022-12-21 21:51:47,258 - Utils - INFO - ...... subset to store [Conv_53, BatchNormalization_54]\n",
      "2022-12-21 21:51:47,258 - Utils - INFO - ...... subset to store [Conv_59, BatchNormalization_60]\n",
      "2022-12-21 21:51:47,258 - Utils - INFO - ...... subset to store [Conv_62, BatchNormalization_63]\n",
      "2022-12-21 21:51:47,259 - Utils - INFO - ...... subset to store [Conv_55, BatchNormalization_56]\n",
      "2022-12-21 21:51:47,259 - Utils - INFO - ...... subset to store [Conv_39, BatchNormalization_40]\n",
      "2022-12-21 21:51:47,260 - Utils - INFO - ...... subset to store [Conv_23, BatchNormalization_24]\n"
     ]
    }
   ],
   "source": [
    "_ = fold_all_batch_norms(model, input_shapes=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d66e0d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = AdaroundParameters(data_loader=val_loader, num_batches=2000//val_loader.batch_size, default_num_iterations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c812d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.rand(1, 3, 224, 224).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44d169d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./imagenet_w4/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66029d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "2022-12-21 21:51:51,361 - Quant - INFO - No config file provided, defaulting to config file at /usr/local/lib/python3.8/dist-packages/aimet_common/quantsim_config/default_config.json\n",
      "2022-12-21 21:51:51,379 - Quant - INFO - Unsupported op type Squeeze\n",
      "2022-12-21 21:51:51,380 - Quant - INFO - Unsupported op type Pad\n",
      "2022-12-21 21:51:51,380 - Quant - INFO - Unsupported op type Mean\n",
      "2022-12-21 21:51:51,384 - Utils - INFO - ...... subset to store [Conv_0, Relu_1]\n",
      "2022-12-21 21:51:51,384 - Utils - INFO - ...... subset to store [Conv_0, Relu_1]\n",
      "2022-12-21 21:51:51,385 - Utils - INFO - ...... subset to store [Conv_3, Relu_4]\n",
      "2022-12-21 21:51:51,385 - Utils - INFO - ...... subset to store [Conv_3, Relu_4]\n",
      "2022-12-21 21:51:51,385 - Utils - INFO - ...... subset to store [Add_6, Relu_7]\n",
      "2022-12-21 21:51:51,386 - Utils - INFO - ...... subset to store [Add_6, Relu_7]\n",
      "2022-12-21 21:51:51,386 - Utils - INFO - ...... subset to store [Conv_8, Relu_9]\n",
      "2022-12-21 21:51:51,387 - Utils - INFO - ...... subset to store [Conv_8, Relu_9]\n",
      "2022-12-21 21:51:51,387 - Utils - INFO - ...... subset to store [Add_11, Relu_12]\n",
      "2022-12-21 21:51:51,387 - Utils - INFO - ...... subset to store [Add_11, Relu_12]\n",
      "2022-12-21 21:51:51,388 - Utils - INFO - ...... subset to store [Conv_13, Relu_14]\n",
      "2022-12-21 21:51:51,388 - Utils - INFO - ...... subset to store [Conv_13, Relu_14]\n",
      "2022-12-21 21:51:51,389 - Utils - INFO - ...... subset to store [Add_17, Relu_18]\n",
      "2022-12-21 21:51:51,389 - Utils - INFO - ...... subset to store [Add_17, Relu_18]\n",
      "2022-12-21 21:51:51,389 - Utils - INFO - ...... subset to store [Conv_19, Relu_20]\n",
      "2022-12-21 21:51:51,390 - Utils - INFO - ...... subset to store [Conv_19, Relu_20]\n",
      "2022-12-21 21:51:51,390 - Utils - INFO - ...... subset to store [Add_22, Relu_23]\n",
      "2022-12-21 21:51:51,391 - Utils - INFO - ...... subset to store [Add_22, Relu_23]\n",
      "2022-12-21 21:51:51,391 - Utils - INFO - ...... subset to store [Conv_24, Relu_25]\n",
      "2022-12-21 21:51:51,392 - Utils - INFO - ...... subset to store [Conv_24, Relu_25]\n",
      "2022-12-21 21:51:51,392 - Utils - INFO - ...... subset to store [Add_28, Relu_29]\n",
      "2022-12-21 21:51:51,392 - Utils - INFO - ...... subset to store [Add_28, Relu_29]\n",
      "2022-12-21 21:51:51,393 - Utils - INFO - ...... subset to store [Conv_30, Relu_31]\n",
      "2022-12-21 21:51:51,393 - Utils - INFO - ...... subset to store [Conv_30, Relu_31]\n",
      "2022-12-21 21:51:51,394 - Utils - INFO - ...... subset to store [Add_33, Relu_34]\n",
      "2022-12-21 21:51:51,394 - Utils - INFO - ...... subset to store [Add_33, Relu_34]\n",
      "2022-12-21 21:51:51,394 - Utils - INFO - ...... subset to store [Conv_35, Relu_36]\n",
      "2022-12-21 21:51:51,395 - Utils - INFO - ...... subset to store [Conv_35, Relu_36]\n",
      "2022-12-21 21:51:51,395 - Utils - INFO - ...... subset to store [Add_39, Relu_40]\n",
      "2022-12-21 21:51:51,396 - Utils - INFO - ...... subset to store [Add_39, Relu_40]\n",
      "2022-12-21 21:51:51,396 - Utils - INFO - ...... subset to store [Conv_41, Relu_42]\n",
      "2022-12-21 21:51:51,396 - Utils - INFO - ...... subset to store [Conv_41, Relu_42]\n",
      "2022-12-21 21:51:51,397 - Utils - INFO - ...... subset to store [Add_44, Relu_45]\n",
      "2022-12-21 21:51:51,397 - Utils - INFO - ...... subset to store [Add_44, Relu_45]\n",
      "2022-12-21 21:52:06,303 - Utils - INFO - Caching 4 batches from data loader at path location: /tmp/adaround/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 21:52:17,590 - Quant - INFO - Started Optimizing weight rounding of module: conv1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 21:53:14,646 - Quant - INFO - Started Optimizing weight rounding of module: layer1.0.conv1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 21:53:46,338 - Quant - INFO - Started Optimizing weight rounding of module: layer1.0.conv2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 21:54:15,999 - Quant - INFO - Started Optimizing weight rounding of module: layer1.1.conv1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 21:54:48,119 - Quant - INFO - Started Optimizing weight rounding of module: layer1.1.conv2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 21:55:18,132 - Quant - INFO - Started Optimizing weight rounding of module: layer2.0.conv1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 21:55:46,201 - Quant - INFO - Started Optimizing weight rounding of module: layer2.0.conv2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 21:56:09,770 - Quant - INFO - Started Optimizing weight rounding of module: layer2.0.downsample.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 21:56:34,446 - Quant - INFO - Started Optimizing weight rounding of module: layer2.1.conv1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 21:56:58,910 - Quant - INFO - Started Optimizing weight rounding of module: layer2.1.conv2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 21:57:22,730 - Quant - INFO - Started Optimizing weight rounding of module: layer3.0.conv1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 21:57:46,916 - Quant - INFO - Started Optimizing weight rounding of module: layer3.0.conv2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 21:58:09,051 - Quant - INFO - Started Optimizing weight rounding of module: layer3.0.downsample.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 21:58:33,840 - Quant - INFO - Started Optimizing weight rounding of module: layer3.1.conv1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 21:58:56,642 - Quant - INFO - Started Optimizing weight rounding of module: layer3.1.conv2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 21:59:19,093 - Quant - INFO - Started Optimizing weight rounding of module: layer4.0.conv1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 21:59:44,111 - Quant - INFO - Started Optimizing weight rounding of module: layer4.0.conv2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 22:00:13,225 - Quant - INFO - Started Optimizing weight rounding of module: layer4.0.downsample.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 22:00:35,344 - Quant - INFO - Started Optimizing weight rounding of module: layer4.1.conv1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 22:01:04,883 - Quant - INFO - Started Optimizing weight rounding of module: layer4.1.conv2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 22:01:34,193 - Quant - INFO - Started Optimizing weight rounding of module: fc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [09:37<00:00,  8.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 22:01:55,316 - Quant - INFO - Deleting model inputs from location: /tmp/adaround/\n",
      "2022-12-21 22:01:55,421 - Quant - INFO - Completed Adarounding Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "ada_model = Adaround.apply_adaround(model, dummy_input, params,\n",
    "                                    path=\"imagenet_w4\", \n",
    "                                    filename_prefix='adaround', \n",
    "                                    default_param_bw=4,\n",
    "                                    default_quant_scheme=QuantScheme.post_training_tf_enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d366f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 22:01:55,657 - Quant - INFO - No config file provided, defaulting to config file at /usr/local/lib/python3.8/dist-packages/aimet_common/quantsim_config/default_config.json\n",
      "2022-12-21 22:01:55,674 - Quant - INFO - Unsupported op type Squeeze\n",
      "2022-12-21 22:01:55,675 - Quant - INFO - Unsupported op type Pad\n",
      "2022-12-21 22:01:55,675 - Quant - INFO - Unsupported op type Mean\n",
      "2022-12-21 22:01:55,678 - Utils - INFO - ...... subset to store [Conv_0, Relu_1]\n",
      "2022-12-21 22:01:55,679 - Utils - INFO - ...... subset to store [Conv_0, Relu_1]\n",
      "2022-12-21 22:01:55,679 - Utils - INFO - ...... subset to store [Conv_3, Relu_4]\n",
      "2022-12-21 22:01:55,680 - Utils - INFO - ...... subset to store [Conv_3, Relu_4]\n",
      "2022-12-21 22:01:55,680 - Utils - INFO - ...... subset to store [Add_6, Relu_7]\n",
      "2022-12-21 22:01:55,681 - Utils - INFO - ...... subset to store [Add_6, Relu_7]\n",
      "2022-12-21 22:01:55,681 - Utils - INFO - ...... subset to store [Conv_8, Relu_9]\n",
      "2022-12-21 22:01:55,681 - Utils - INFO - ...... subset to store [Conv_8, Relu_9]\n",
      "2022-12-21 22:01:55,682 - Utils - INFO - ...... subset to store [Add_11, Relu_12]\n",
      "2022-12-21 22:01:55,682 - Utils - INFO - ...... subset to store [Add_11, Relu_12]\n",
      "2022-12-21 22:01:55,683 - Utils - INFO - ...... subset to store [Conv_13, Relu_14]\n",
      "2022-12-21 22:01:55,683 - Utils - INFO - ...... subset to store [Conv_13, Relu_14]\n",
      "2022-12-21 22:01:55,683 - Utils - INFO - ...... subset to store [Add_17, Relu_18]\n",
      "2022-12-21 22:01:55,684 - Utils - INFO - ...... subset to store [Add_17, Relu_18]\n",
      "2022-12-21 22:01:55,684 - Utils - INFO - ...... subset to store [Conv_19, Relu_20]\n",
      "2022-12-21 22:01:55,685 - Utils - INFO - ...... subset to store [Conv_19, Relu_20]\n",
      "2022-12-21 22:01:55,685 - Utils - INFO - ...... subset to store [Add_22, Relu_23]\n",
      "2022-12-21 22:01:55,685 - Utils - INFO - ...... subset to store [Add_22, Relu_23]\n",
      "2022-12-21 22:01:55,686 - Utils - INFO - ...... subset to store [Conv_24, Relu_25]\n",
      "2022-12-21 22:01:55,686 - Utils - INFO - ...... subset to store [Conv_24, Relu_25]\n",
      "2022-12-21 22:01:55,687 - Utils - INFO - ...... subset to store [Add_28, Relu_29]\n",
      "2022-12-21 22:01:55,687 - Utils - INFO - ...... subset to store [Add_28, Relu_29]\n",
      "2022-12-21 22:01:55,688 - Utils - INFO - ...... subset to store [Conv_30, Relu_31]\n",
      "2022-12-21 22:01:55,688 - Utils - INFO - ...... subset to store [Conv_30, Relu_31]\n",
      "2022-12-21 22:01:55,688 - Utils - INFO - ...... subset to store [Add_33, Relu_34]\n",
      "2022-12-21 22:01:55,689 - Utils - INFO - ...... subset to store [Add_33, Relu_34]\n",
      "2022-12-21 22:01:55,689 - Utils - INFO - ...... subset to store [Conv_35, Relu_36]\n",
      "2022-12-21 22:01:55,690 - Utils - INFO - ...... subset to store [Conv_35, Relu_36]\n",
      "2022-12-21 22:01:55,690 - Utils - INFO - ...... subset to store [Add_39, Relu_40]\n",
      "2022-12-21 22:01:55,690 - Utils - INFO - ...... subset to store [Add_39, Relu_40]\n",
      "2022-12-21 22:01:55,691 - Utils - INFO - ...... subset to store [Conv_41, Relu_42]\n",
      "2022-12-21 22:01:55,691 - Utils - INFO - ...... subset to store [Conv_41, Relu_42]\n",
      "2022-12-21 22:01:55,692 - Utils - INFO - ...... subset to store [Add_44, Relu_45]\n",
      "2022-12-21 22:01:55,692 - Utils - INFO - ...... subset to store [Add_44, Relu_45]\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.rand(1, 3, 224, 224)    # Shape for each ImageNet sample is (3 channels) x (224 height) x (224 width)\n",
    "dummy_input = dummy_input.cuda()\n",
    "\n",
    "sim = QuantizationSimModel(model=ada_model,\n",
    "                           quant_scheme=QuantScheme.post_training_tf_enhanced,\n",
    "                           dummy_input=dummy_input,\n",
    "                           default_output_bw=4,\n",
    "                           default_param_bw=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b77c347d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 22:01:55,700 - Quant - INFO - Setting quantization encodings for parameter: conv1.weight\n",
      "2022-12-21 22:01:55,700 - Quant - INFO - Freezing quantization encodings for parameter: conv1.weight\n",
      "2022-12-21 22:01:55,701 - Quant - INFO - Setting quantization encodings for parameter: layer1.0.conv1.weight\n",
      "2022-12-21 22:01:55,701 - Quant - INFO - Freezing quantization encodings for parameter: layer1.0.conv1.weight\n",
      "2022-12-21 22:01:55,702 - Quant - INFO - Setting quantization encodings for parameter: layer1.0.conv2.weight\n",
      "2022-12-21 22:01:55,702 - Quant - INFO - Freezing quantization encodings for parameter: layer1.0.conv2.weight\n",
      "2022-12-21 22:01:55,703 - Quant - INFO - Setting quantization encodings for parameter: layer1.1.conv1.weight\n",
      "2022-12-21 22:01:55,703 - Quant - INFO - Freezing quantization encodings for parameter: layer1.1.conv1.weight\n",
      "2022-12-21 22:01:55,704 - Quant - INFO - Setting quantization encodings for parameter: layer1.1.conv2.weight\n",
      "2022-12-21 22:01:55,704 - Quant - INFO - Freezing quantization encodings for parameter: layer1.1.conv2.weight\n",
      "2022-12-21 22:01:55,705 - Quant - INFO - Setting quantization encodings for parameter: layer2.0.conv1.weight\n",
      "2022-12-21 22:01:55,705 - Quant - INFO - Freezing quantization encodings for parameter: layer2.0.conv1.weight\n",
      "2022-12-21 22:01:55,705 - Quant - INFO - Setting quantization encodings for parameter: layer2.0.conv2.weight\n",
      "2022-12-21 22:01:55,706 - Quant - INFO - Freezing quantization encodings for parameter: layer2.0.conv2.weight\n",
      "2022-12-21 22:01:55,706 - Quant - INFO - Setting quantization encodings for parameter: layer2.0.downsample.0.weight\n",
      "2022-12-21 22:01:55,707 - Quant - INFO - Freezing quantization encodings for parameter: layer2.0.downsample.0.weight\n",
      "2022-12-21 22:01:55,707 - Quant - INFO - Setting quantization encodings for parameter: layer2.1.conv1.weight\n",
      "2022-12-21 22:01:55,707 - Quant - INFO - Freezing quantization encodings for parameter: layer2.1.conv1.weight\n",
      "2022-12-21 22:01:55,708 - Quant - INFO - Setting quantization encodings for parameter: layer2.1.conv2.weight\n",
      "2022-12-21 22:01:55,708 - Quant - INFO - Freezing quantization encodings for parameter: layer2.1.conv2.weight\n",
      "2022-12-21 22:01:55,709 - Quant - INFO - Setting quantization encodings for parameter: layer3.0.conv1.weight\n",
      "2022-12-21 22:01:55,709 - Quant - INFO - Freezing quantization encodings for parameter: layer3.0.conv1.weight\n",
      "2022-12-21 22:01:55,710 - Quant - INFO - Setting quantization encodings for parameter: layer3.0.conv2.weight\n",
      "2022-12-21 22:01:55,710 - Quant - INFO - Freezing quantization encodings for parameter: layer3.0.conv2.weight\n",
      "2022-12-21 22:01:55,710 - Quant - INFO - Setting quantization encodings for parameter: layer3.0.downsample.0.weight\n",
      "2022-12-21 22:01:55,711 - Quant - INFO - Freezing quantization encodings for parameter: layer3.0.downsample.0.weight\n",
      "2022-12-21 22:01:55,711 - Quant - INFO - Setting quantization encodings for parameter: layer3.1.conv1.weight\n",
      "2022-12-21 22:01:55,712 - Quant - INFO - Freezing quantization encodings for parameter: layer3.1.conv1.weight\n",
      "2022-12-21 22:01:55,712 - Quant - INFO - Setting quantization encodings for parameter: layer3.1.conv2.weight\n",
      "2022-12-21 22:01:55,713 - Quant - INFO - Freezing quantization encodings for parameter: layer3.1.conv2.weight\n",
      "2022-12-21 22:01:55,713 - Quant - INFO - Setting quantization encodings for parameter: layer4.0.conv1.weight\n",
      "2022-12-21 22:01:55,713 - Quant - INFO - Freezing quantization encodings for parameter: layer4.0.conv1.weight\n",
      "2022-12-21 22:01:55,714 - Quant - INFO - Setting quantization encodings for parameter: layer4.0.conv2.weight\n",
      "2022-12-21 22:01:55,714 - Quant - INFO - Freezing quantization encodings for parameter: layer4.0.conv2.weight\n",
      "2022-12-21 22:01:55,715 - Quant - INFO - Setting quantization encodings for parameter: layer4.0.downsample.0.weight\n",
      "2022-12-21 22:01:55,715 - Quant - INFO - Freezing quantization encodings for parameter: layer4.0.downsample.0.weight\n",
      "2022-12-21 22:01:55,716 - Quant - INFO - Setting quantization encodings for parameter: layer4.1.conv1.weight\n",
      "2022-12-21 22:01:55,716 - Quant - INFO - Freezing quantization encodings for parameter: layer4.1.conv1.weight\n",
      "2022-12-21 22:01:55,716 - Quant - INFO - Setting quantization encodings for parameter: layer4.1.conv2.weight\n",
      "2022-12-21 22:01:55,717 - Quant - INFO - Freezing quantization encodings for parameter: layer4.1.conv2.weight\n",
      "2022-12-21 22:01:55,717 - Quant - INFO - Setting quantization encodings for parameter: fc.weight\n",
      "2022-12-21 22:01:55,718 - Quant - INFO - Freezing quantization encodings for parameter: fc.weight\n"
     ]
    }
   ],
   "source": [
    "sim.set_and_freeze_param_encodings(encoding_path=os.path.join(\"imagenet_w4\", 'adaround.encodings'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba6bf73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "sim.compute_encodings(forward_pass_callback=pass_calibration_data,\n",
    "                      forward_pass_callback_args=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfe4cfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 4.29 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:12<00:00,  1.32s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22722"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "accuracy(sim.model, test_loader, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad3f989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
